{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install -U scikit-learn\n","!pip install keras-nlp tensorflow --upgrade\n","!pip install seaborn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Import the necessary packages\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","import transformers\n","from transformers import TFAutoModel, AutoTokenizer\n","from tqdm.notebook import tqdm\n","from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n","\n","from keras import backend as K\n"]},{"cell_type":"markdown","metadata":{},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["'''\n","Method to encode the texts using the tokenizer in a quick way\n","\n","Params:\n","    texts  - inpt text to be encoded\n","    tokenizer - tokenizer function to be used\n","    chunk_size - the size of the chunk \n","    maxlen - maximum length of the chunks\n","    \n","Returns:\n","    numpy array of encoded texts\n","'''\n","def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n","    tokenizer.enable_truncation(max_length=maxlen)\n","    tokenizer.enable_padding\n","    ng(max_length=maxlen)\n","    all_ids = []\n","    \n","    for i in tqdm(range(0, len(texts), chunk_size)):\n","        text_chunk = texts[i:i+chunk_size].tolist()\n","        encs = tokenizer.encode_batch(text_chunk)\n","        all_ids.extend([enc.ids for enc in encs])\n","    \n","    return np.array(all_ids)\n","\n","'''\n","Method to encode the texts using a tokenizer in a normal way\n","\n","Params:\n","    texts  - inpt text to be encoded\n","    tokenizer - tokenizer function to be used\n","    maxlen - maximum length of the chunks\n","Returns:\n","    numpy array of encoded texts\n","'''\n","def regular_encode(texts, tokenizer, maxlen=512):\n","    enc_di = tokenizer.batch_encode_plus(\n","        texts, \n","        return_token_type_ids=False,\n","        pad_to_max_length=True,\n","        max_length=maxlen\n","    )\n","    \n","    return np.array(enc_di['input_ids'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''\n","Method to calculate the recall score\n","\n","Params:\n","    y_true - original y values\n","    y_pred - predicted y values\n","Returns:\n","    recall score\n","'''\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","'''\n","Method to calculate the precision score\n","\n","Params:\n","    y_true - original y values\n","    y_pred - predicted y values\n","Returns:\n","    precision score    \n","'''\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","'''\n","Method to calculate the f1 score\n","\n","Params:\n","    y_true - original y values\n","    y_pred - predicted y values\n","Returns:\n","    f1 score    \n","'''\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''\n","Method to build the stacked Transformer architecture.\n","\n","Params:\n","    num_heads - number of multi-headed attention blocks\n","    feed_forward_dim - number of neurons in the feed forward network\n","    attention_dropout - dropout percent after attention layer\n","    feed_forward_dropout - dropout percent after feed forward layer\n","Returns:\n","    final model\n","'''\n","def vanilla_transformer(num_heads=8, feed_forward_dim=768, attention_dropout=0.1, feed_forward_dropout=0.1):\n","    attention_layer = tf.keras.layers.MultiHeadAttention(\n","        num_heads=num_heads, key_dim=sequence_output.shape[-1])(sequence_output, sequence_output)\n","    attention_layer = Dropout(attention_dropout)(attention_layer)\n","    attention_layer = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(attention_layer + sequence_output)\n","    \n","    feed_forward_layer = Dense(feed_forward_dim, activation='relu')(attention_layer)\n","    feed_forward_layer = Dropout(feed_forward_dropout)(feed_forward_layer)\n","    transformer_layer = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(feed_forward_layer + attention_layer)\n","    \n","    return transformer_layer\n","\n","'''\n","Method to build the stacked Transformer architecture.\n","\n","Params:\n","    transformer - input transformer architecture\n","    max_len - maximum length of a chunk\n","Returns:\n","    final model\n","'''\n","def build_model(transformer, max_len=512):\n","    \n","    # define the input layer\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    \n","    # define the feature encoder using XLM-RoBERTa model\n","    sequence_output = transformer(input_word_ids)[0]\n","    \n","    transformer_layer = vanilla_transformer()(sequence_output)\n","\n","    cls_token = transformer_layer[:, 0, :]\n","    x1 = Dense(256, activation = 'relu')(cls_token)\n","    x1 = Dense(128, activation = 'relu')(x1)\n","    x1 = Dense(64, activation = 'relu')(x1)\n","    x1 = Dense(4, activation = 'relu')(x1)\n","    x1 = Dense(32, activation = 'relu')(x1)\n","    x1 = Dense(16, activation = 'relu')(x1)\n","    out = Dense(2, activation='softmax')(x1)\n","    \n","    model = Model(inputs=input_word_ids, outputs=out)\n","    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy', recall_m, precision_m, f1_m])\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## TPU Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Detect hardware, return appropriate distribution strategy\n","try:\n","    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n","    # set: this is always the case on Kaggle.\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","    strategy = tf.distribute.get_strategy()\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AUTO = tf.data.experimental.AUTOTUNE\n","\n","# Configuration\n","EPOCHS = 1\n","BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","MAX_LEN = 275\n","MODEL = 'jplu/tf-xlm-roberta-base'"]},{"cell_type":"markdown","metadata":{},"source":["## Create fast tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# First load the real tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)"]},{"cell_type":"markdown","metadata":{},"source":["## Load text data into memory"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n","train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n","train2.toxic = train2.toxic.round().astype(int)\n","\n","valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n","test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n","\n","train = pd.concat([\n","    train1[['comment_text', 'toxic']],\n","    train2[['comment_text', 'toxic']].query('toxic==1'),\n","    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n","])\n","\n","train = train.groupby('toxic').apply(\n","    lambda x: x.sample(frac=0.5)\n",")\n","\n","train = train.droplevel(0)\n","\n","print(\"length of train data : \",len(train.index))\n","print(\"length of valid data : \", len(valid.index))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Subcample the data\n","\n","nontoxic = train[train['toxic']==0]\n","toxic = train[train[\"toxic\"]==1]\n","\n","new_nontoxic = nontoxic.sample(toxic.shape[0])\n","new_nontoxic.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.utils import shuffle\n","train = shuffle(pd.concat([toxic, new_nontoxic]))\n","train_unsampled = shuffle(pd.concat([toxic, nontoxic]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# delete the unneccary variabled from memory to save ram space.\n","del train1\n","del train2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# preprocess the data into x_train, x_valid and x_test\n","%%time \n","\n","x_train = regular_encode(train.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n","x_valid = regular_encode(valid.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n","\n","x_test = regular_encode(test.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n","\n","y_train = train.toxic.values.tolist()\n","y_valid = valid.toxic.values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# preprocess the data into x_train_unsampled\n","\n","%%time \n","\n","x_train_unsampled = regular_encode(train_unsampled.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n","y_train_unsampled = train_unsampled.toxic.values.tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_train = tf.keras.utils.to_categorical(y_train, num_classes = 2)\n","y_train_unsampled = tf.keras.utils.to_categorical(y_train_unsampled, num_classes = 2)\n","y_valid = tf.keras.utils.to_categorical(y_valid, num_classes = 2)"]},{"cell_type":"markdown","metadata":{},"source":["## Build datasets objects"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((x_train, y_train))\n","    .repeat()\n","    .shuffle(2048)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTO)\n",")\n","\n","valid_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((x_valid, y_valid))\n","    .batch(BATCH_SIZE)\n","    .cache()\n","    .prefetch(AUTO)\n",")\n","\n","test_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices(x_test)\n","    .batch(BATCH_SIZE)\n",")\n","\n","n_steps = x_train.shape[0] // BATCH_SIZE\n","n_steps_valid = x_valid.shape[0] // BATCH_SIZE\n","\n","del x_train\n","del y_train"]},{"cell_type":"markdown","metadata":{},"source":["## Load model into the TPU"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["id2label = {0: \"NONTOXIC\", 1: \"TOXIC\"}\n","label2id = {\"NONTOXIC\": 0, \"TOXIC\": 1}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","with strategy.scope():\n","    transformer_layer = TFAutoModel.from_pretrained(MODEL, num_labels=2, id2label=id2label, label2id=label2id)\n","    \n","    model = build_model(transformer_layer, max_len=MAX_LEN)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Train Model"]},{"cell_type":"markdown","metadata":{},"source":["First, we train on the subset of the training set, which is completely in English."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["EPOCHS = 10\n","train_history = model.fit(\n","    train_dataset,\n","    steps_per_epoch=n_steps,\n","    validation_data=valid_dataset,\n","    epochs=EPOCHS\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the `validation` set, which is significantly smaller but contains a mixture of different languages."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_history_2 = model.fit(\n","    valid_dataset.repeat(),\n","    steps_per_epoch=n_steps_valid,\n","    epochs=3\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import itertools\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix\n","def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting normalize=True.\n","    \"\"\"\n","    plt.figure(figsize=(10,10))\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        cm = np.around(cm, decimals=2)\n","        cm[np.isnan(cm)] = 0.0\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["###Overall Model\n","target_names = ['Toxic', 'Non-Toxic']\n","\n","Y_pred = model.predict(x_valid)\n","y_preds = np.argmax(Y_pred, axis=1)\n","print('Confusion Matrix')\n","rounded_labels=np.argmax(y_valid, axis=1)\n","cm = confusion_matrix(y_true = rounded_labels, y_pred = y_preds)\n","print(cm)\n","plot_confusion_matrix(cm, target_names, title='Confusion Matrix')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import seaborn as sns\n","def plot_confusion_matrix2(y_true, y_pred, classes):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, xticklabels=classes, yticklabels=classes, fmt='g')\n","    plt.xlabel('Predicted labels')\n","    plt.ylabel('True labels')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","plot_confusion_matrix2(y_true = rounded_labels, y_pred = y_preds, classes=target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import precision_recall_curve\n","precision = dict()\n","recall = dict()\n","for i in range(2):\n","    precision[i], recall[i], _ = precision_recall_curve(y_valid[:,i],\n","                                                        Y_pred[:, i])\n","    if (i==0):\n","      plt.plot(recall[i], precision[i], lw=2, label='Toxic')\n","    elif (i==1):\n","      plt.plot(recall[i], precision[i], lw=2, label='NonToxic')\n","    \n","plt.xlabel(\"recall\")\n","plt.ylabel(\"precision\")\n","plt.legend(loc=\"best\")\n","plt.title(\"precision vs. recall curve\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["axes = plt.gca()\n","acc = train_history_2.history['accuracy']\n","loss = train_history_2.history['loss']\n","epochs = range(1, len(acc) + 1)\n","#Train and validation accuracy\n","plt.plot(epochs, acc, 'b', label='Training accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.title('Training accuracy and Validation accuracy')\n","\n","plt.legend()\n","\n","plt.figure()\n","#Train and validation loss\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title('Training loss and Validation loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["axes = plt.gca()\n","acc = train_history.history['accuracy']\n","val_acc = train_history.history['val_accuracy']\n","loss = train_history.history['loss']\n","val_loss = train_history.history['val_loss']\n","epochs = range(1, len(acc) + 1)\n","#Train and validation accuracy\n","plt.plot(epochs, acc, 'b', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.title('Training accuracy and Validation accuracy')\n","\n","plt.legend()\n","\n","plt.figure()\n","#Train and validation loss\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title('Training loss and Validation loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.evaluate(valid_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report\n","import time\n","import numpy as np\n","start_time = time.time()\n","test_predictions = model.predict(x_valid)\n","# Comparing the predictions to actual forest cover types for the test rows\n","# test is the data right after splitting into train, test and val (shuffle was false in dataset so the order will match)\n","rounded_labels=np.argmax(y_valid, axis=1)\n","test_predictions = np.argmax(test_predictions, axis=1)\n","print(classification_report(rounded_labels,test_predictions))\n","print(\"Time taken to predict the model \" + str(time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del train_dataset\n","del valid_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n","sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n","submission = pd.read_csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# x_test = regular_encode(test.content.values.tolist()[0:63812], tokenizer, maxlen=MAX_LEN)\n","x_test = regular_encode(test.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n","\n","test_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices(x_test)\n","    .batch(BATCH_SIZE)\n",")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
