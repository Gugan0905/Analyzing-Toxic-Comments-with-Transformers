## Analyzing Toxic Comment Classification Performance on ML and Transformer Algorithms

###  Teammates
* Gugan Kathiresan - NUID 002756523
* Maria Anson - NUID 002931419
* Aditya Shanmugham - NUID : 002738073

### Project Files
- "Machine Learning Models.ipynb" : End to end notebook for data loader and modelling K Nearest Neighbors, Random Forests and XG Boost for Toxic Comment Classification
- "TransformerWithFFNN.ipynb" : End to end notebook for data loader and modelling a variation of the BERT transformer with a Feed Forward Neural Network Classifier
- "xlm-roberta-with-stacked-transformer.ipynb" : End to end notebook for data loader and modelling a unique stacked Ensemble of the RoBERTa transformer

```Full Report Available in Repo```

## Abstract
Toxic comment classification is a significant niche in the
field of natural language processing owing to its vast
real-world applications. Popularly used in the field of
digital media, toxic comment classification is vital towards
maintaining a decorous and peaceful society. <br> The use of
natural language processing algorithms to accelerate toxic
comment classification across various data sources,
contextual situations and languages have been attempted by
researchers of late. <br> This study proposes a comparative
analysis of fundamental machine learning algorithms and
paradigm transformer architectures to classify multilingual
toxic comments. The methods employed in this study include
k-nearest neighbors, xgboost and transformers architectures
like BERT, etc. <br> These methods are compared to verify their
performance and efficiency in a contextual manner. From
the results obtained, it was inferred that transformers offer
better quality classification compared to the basic machine
learning models. <br> The ability of transformers to handle
multilingual data makes it a viable solution that can be
deployed in real-world applications. Future improvements
could be to deploy.
